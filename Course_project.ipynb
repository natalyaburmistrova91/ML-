{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цель данной работы - при помощи комментария, отзыва или любого другого текста, написанного пользователем, определять социотип по типологии Майерс-Бриггс. Это поможет лучше понимать пользователей и возможно поможет как дополнительный признак настроить рекомендательную систему."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Источник  https://habr.com/ru/company/surfingbird/blog/230103/\n",
    "\n",
    "kaggle https://www.kaggle.com/datasnaek/mbti-type?select=mbti_1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8675, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"'http://www.youtube.com/watch?v=qsXHcwe3krw|||http://41.media.tumblr.com/tumblr_lfouy03PMA1qa1rooo1_500.jpg|||enfp and intj moments  https://www.youtube.com/watch?v=iz7lE1g4XM4  sportscenter not top ten plays  https://www.youtube.com/watch?v=uCdfze1etec  pranks|||What has been the most life-changing experience in your life?|||http://www.youtube.com/watch?v=vXZeYwwRDw8   http://www.youtube.com/watch?v=u8ejam5DP3E  On repeat for most of today.|||May the PerC Experience immerse you.|||The last thing my INFJ friend posted on his facebook before committing suicide the next day. Rest in peace~   http://vimeo.com/22842206|||Hello ENFJ7. Sorry to hear of your distress. It's only natural for a relationship to not be perfection all the time in every moment of existence. Try to figure the hard times as times of growth, as...|||84389  84390  http://wallpaperpassion.com/upload/23700/friendship-boy-and-girl-wallpaper.jpg  http://assets.dornob.com/wp-content/uploads/2010/04/round-home-design.jpg ...|||Welcome and stuff.|||http://playeressence.com/wp-content/uploads/2013/08/RED-red-the-pokemon-master-32560474-450-338.jpg  Game. Set. Match.|||Prozac, wellbrutin, at least thirty minutes of moving your legs (and I don't mean moving them while sitting in your same desk chair), weed in moderation (maybe try edibles as a healthier alternative...|||Basically come up with three items you've determined that each type (or whichever types you want to do) would more than likely use, given each types' cognitive functions and whatnot, when left by...|||All things in moderation.  Sims is indeed a video game, and a good one at that. Note: a good one at that is somewhat subjective in that I am not completely promoting the death of any given Sim...|||Dear ENFP:  What were your favorite video games growing up and what are your now, current favorite video games? :cool:|||https://www.youtube.com/watch?v=QyPqT8umzmY|||It appears to be too late. :sad:|||There's someone out there for everyone.|||Wait... I thought confidence was a good thing.|||I just cherish the time of solitude b/c i revel within my inner world more whereas most other time i'd be workin... just enjoy the me time while you can. Don't worry, people will always be around to...|||Yo entp ladies... if you're into a complimentary personality,well, hey.|||... when your main social outlet is xbox live conversations and even then you verbally fatigue quickly.|||http://www.youtube.com/watch?v=gDhy7rdfm14  I really dig the part from 1:46 to 2:50|||http://www.youtube.com/watch?v=msqXffgh7b8|||Banned because this thread requires it of me.|||Get high in backyard, roast and eat marshmellows in backyard while conversing over something intellectual, followed by massages and kisses.|||http://www.youtube.com/watch?v=Mw7eoU3BMbE|||http://www.youtube.com/watch?v=4V2uYORhQOk|||http://www.youtube.com/watch?v=SlVmgFQQ0TI|||Banned for too many b's in that sentence. How could you! Think of the B!|||Banned for watching movies in the corner with the dunces.|||Banned because Health class clearly taught you nothing about peer pressure.|||Banned for a whole host of reasons!|||http://www.youtube.com/watch?v=IRcrv41hgz4|||1) Two baby deer on left and right munching on a beetle in the middle.  2) Using their own blood, two cavemen diary today's latest happenings on their designated cave diary wall.  3) I see it as...|||a pokemon world  an infj society  everyone becomes an optimist|||49142|||http://www.youtube.com/watch?v=ZRCEq_JFeFM|||http://discovermagazine.com/2012/jul-aug/20-things-you-didnt-know-about-deserts/desert.jpg|||http://oyster.ignimgs.com/mediawiki/apis.ign.com/pokemon-silver-version/d/dd/Ditto.gif|||http://www.serebii.net/potw-dp/Scizor.jpg|||Not all artists are artists because they draw. It's the idea that counts in forming something of your own... like a signature.|||Welcome to the robot ranks, person who downed my self-esteem cuz I'm not an avid signature artist like herself. :proud:|||Banned for taking all the room under my bed. Ya gotta learn to share with the roaches.|||http://www.youtube.com/watch?v=w8IgImn57aQ|||Banned for being too much of a thundering, grumbling kind of storm... yep.|||Ahh... old high school music I haven't heard in ages.   http://www.youtube.com/watch?v=dcCRUPCdB1w|||I failed a public speaking class a few years ago and I've sort of learned what I could do better were I to be in that position again. A big part of my failure was just overloading myself with too...|||I like this person's mentality. He's a confirmed INTJ by the way. http://www.youtube.com/watch?v=hGKLI-GEc6M|||Move to the Denver area and start a new life for myself.'\""
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import itertools\n",
    "\n",
    "\n",
    "comments = pd.read_csv(\"mbti_1.csv\")\n",
    "print(comments.shape)\n",
    "comments['posts'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы видим в постах много ссылок, лишних знаков, данные не очень чистые"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...\n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat..."
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from gensim.test.utils import common_texts\n",
    "#from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "#предобработка текстов\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "#from nltk.tokenize import word_tokenize\n",
    "\n",
    "from razdel import tokenize # https://github.com/natasha/razdel\n",
    "#!pip install razdel\n",
    "\n",
    "import pymorphy2  # pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n"
     ]
    }
   ],
   "source": [
    "stopword_eng = stopwords.words('english')\n",
    "print(len(stopword_eng))\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дополнительно скачала стоп слова из свободных источников"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1030"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('stop_words_english.txt') as f:\n",
    "    additional_stopwords = [w.strip() for w in f.readlines() if w]\n",
    "stopword_eng += additional_stopwords\n",
    "len(stopword_eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также убрала сами названия типов из текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_types = ['ENFJ', 'ENFP', 'ENTJ', 'ENTP', 'ESFJ', 'ESFP', 'ESTJ', 'ESTP', 'INFJ', 'INFP', 'INTJ', 'INTP', 'ISFJ', 'ISFP', 'ISTJ', 'ISTP']\n",
    "for type in list_of_types:\n",
    "    stopword_eng.append(type.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1046"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopword_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''\n",
    "    очистка текста\n",
    "    \n",
    "    на выходе очищеный текст\n",
    "    \n",
    "    '''\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = text.strip('\\n').strip('\\r').strip('\\t')\n",
    "    text = re.sub(\"-\\s\\r\\n\\|-\\s\\r\\n|\\r\\n\", '', str(text))\n",
    "    text = re.sub(\"\\|\", ' ', str(text))\n",
    "    text = re.sub(\"https?:\\/\\/\\S+\", '', str(text))\n",
    "    text = re.sub(\"[0-9]|[-—.,:;_%©«»?*!@#№$^•·&()]|[+=]|[[]|[]]|[/]|\", '', text)\n",
    "    text = re.sub(r\"\\r\\n\\t|\\n|\\\\s|\\r\\t|\\\\n\", ' ', text)\n",
    "    text = re.sub(r'[\\xad]|[\\s+]', ' ', text.strip())\n",
    "    \n",
    "    #tokens = list(tokenize(text))\n",
    "    #words = [_.text for _ in tokens]\n",
    "    #words = [w for w in words if w not in stopword_ru]\n",
    "    \n",
    "    #return \" \".join(words)\n",
    "    return text\n",
    "\n",
    "cache = {}\n",
    "\n",
    "def lemmatization(text):\n",
    "    '''\n",
    "    лемматизация\n",
    "        [0] если зашел тип не `str` делаем его `str`\n",
    "        [1] токенизация предложения через razdel\n",
    "        [2] проверка есть ли в начале слова '-'\n",
    "        [3] проверка токена с одного символа\n",
    "        [4] проверка есть ли данное слово в кэше\n",
    "        [5] лемматизация слова\n",
    "        [6] проверка на стоп-слова\n",
    "\n",
    "    на выходе лист отлемматизированых токенов\n",
    "    '''\n",
    "\n",
    "    # [0]\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    # [1]\n",
    "    tokens = list(tokenize(text))\n",
    "    words = [_.text for _ in tokens]\n",
    "\n",
    "    words_lem = []\n",
    "    for w in words:\n",
    "        if w[0] == '-': # [2]\n",
    "            w = w[1:]\n",
    "        if len(w)>1: # [3]\n",
    "            if w in cache: # [4]\n",
    "                words_lem.append(cache[w])\n",
    "            else: # [5]\n",
    "                temp_cach = cache[w] = morph.parse(w)[0].normal_form\n",
    "                words_lem.append(temp_cach)\n",
    "    \n",
    "    words_lem_without_stopwords=[i for i in words_lem if not i in stopword_eng] # [6]\n",
    "    \n",
    "    return words_lem_without_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируем на 1 посте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['moments', 'sportscenter', 'plays', 'pranks', 'lifechanging', 'experience', 'life', 'repeat', 'today', 'perc', 'experience', 'immerse', 'friend', 'posted', 'facebook', 'committing', 'suicide', 'day', 'rest', 'peace', 'hear', 'distress', 'natural', 'relationship', 'perfection', 'time', 'moment', 'existence', 'figure', 'hard', 'times', 'times', 'growth', 'stuff', 'game', 'set', 'match', 'prozac', 'wellbrutin', 'minutes', 'moving', 'legs', 'moving', 'sitting', 'desk', 'chair', 'weed', 'moderation', 'edibles', 'healthier', 'alternative', 'basically', 'items', 'determined', 'type', 'types', 'types', 'cognitive', 'functions', 'whatnot', 'left', 'moderation', 'sims', 'video', 'game', 'good', 'note', 'good', 'subjective', 'completely', 'promoting', 'death', 'sim', 'dear', 'favorite', 'video', 'games', 'growing', 'current', 'favorite', 'video', 'games', 'cool', 'appears', 'late', 'sad', 'wait', 'thought', 'confidence', 'good', 'cherish', 'time', 'solitude', 'bc', 'revel', 'time', 'workin', 'enjoy', 'time', 'worry', 'people', 'yo', 'ladies', 'complimentary', 'personalitywell', 'hey', 'main', 'social', 'outlet', 'xbox', 'live', 'conversations', 'verbally', 'fatigue', 'dig', 'banned', 'thread', 'requires', 'high', 'backyard', 'roast', 'eat', 'marshmellows', 'backyard', 'conversing', 'intellectual', 'massages', 'kisses', 'banned', 'sentence', 'banned', 'watching', 'movies', 'corner', 'dunces', 'banned', 'health', 'class', 'taught', 'peer', 'pressure', 'banned', 'host', 'reasons', 'baby', 'deer', 'left', 'munching', 'beetle', 'middle', 'blood', 'cavemen', 'diary', 'today', 'latest', 'happenings', 'designated', 'cave', 'diary', 'wall', 'pokemon', 'society', 'optimist', 'artists', 'artists', 'draw', 'idea', 'counts', 'forming', 'signature', 'robot', 'ranks', 'person', 'downed', 'selfesteem', 'cuz', 'avid', 'signature', 'artist', 'banned', 'room', 'bed', 'ya', 'gotta', 'learn', 'share', 'roaches', 'banned', 'thundering', 'grumbling', 'kind', 'storm', 'yep', 'ahh', 'high', 'school', 'music', 'heard', 'ages', 'failed', 'public', 'speaking', 'class', 'years', 'sort', 'learned', 'position', 'big', 'failure', 'overloading', 'person', 'mentality', 'confirmed', 'denver', 'area', 'start', 'life']\n"
     ]
    }
   ],
   "source": [
    "text = clean_text(comments['posts'][0])\n",
    "text2 = lemmatization(text)\n",
    "print(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 26.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Запускаем очистку текста\n",
    "comments['posts'] = comments['posts'].apply(lambda x: clean_text(x), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Запускаем лемматизацию текста\n",
    "comments['posts'] = comments['posts'].apply(lambda x: lemmatization(x), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [moments, sportscenter, plays, pranks, lifecha...\n",
       "1       [finding, lack, posts, alarming, sex, boring, ...\n",
       "2       [good, blessing, curse, absolutely, positive, ...\n",
       "3       [dear, enjoyed, conversation, day, esoteric, g...\n",
       "4       [fired, silly, misconception, approaching, log...\n",
       "                              ...                        \n",
       "8670    [ixfp, cats, fi, doms, reason, websites, neo, ...\n",
       "8671    [soif, thread, exists, someplace, heck, delete...\n",
       "8672    [questions, purple, pill, pick, winning, lotte...\n",
       "8673    [conflicted, wanting, children, honestly, mate...\n",
       "8674    [long, personalitycafe, changed, bit, good, tu...\n",
       "Name: posts, Length: 8675, dtype: object"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments['posts']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим текст на основе очищенных слов (делаем join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments['posts'] = comments['posts'].apply(lambda x: ' '.join(x), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       moments sportscenter plays pranks lifechanging...\n",
       "1       finding lack posts alarming sex boring positio...\n",
       "2       good blessing curse absolutely positive friend...\n",
       "3       dear enjoyed conversation day esoteric gabbing...\n",
       "4       fired silly misconception approaching logicall...\n",
       "                              ...                        \n",
       "8670    ixfp cats fi doms reason websites neo nazis pe...\n",
       "8671    soif thread exists someplace heck delete ooops...\n",
       "8672    questions purple pill pick winning lottery num...\n",
       "8673    conflicted wanting children honestly maternal ...\n",
       "8674    long personalitycafe changed bit good turn doc...\n",
       "Name: posts, Length: 8675, dtype: object"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments['posts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INFP    1832\n",
       "INFJ    1470\n",
       "INTP    1304\n",
       "INTJ    1091\n",
       "ENTP     685\n",
       "ENFP     675\n",
       "ISTP     337\n",
       "ISFP     271\n",
       "ENTJ     231\n",
       "ISTJ     205\n",
       "ENFJ     190\n",
       "ISFJ     166\n",
       "ESTP      89\n",
       "ESFP      48\n",
       "ESFJ      42\n",
       "ESTJ      39\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments['type'].value_counts() # данные, к сожалению, не сбалансированы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#разделим данные на train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(comments, comments['type'], random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Многоклассовая классификация с LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#соберем наш простой pipeline, но нам понадобится написать класс для выбора нужного поля\n",
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X[self.column]\n",
    "\n",
    "pipeline = Pipeline([('title_selector', FeatureSelector(column='posts')), \n",
    "                     ('title_tfidf', TfidfVectorizer()), \n",
    "                     ('clf', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anakonda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('title_selector', FeatureSelector(column='posts')),\n",
       "                ('title_tfidf',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=N...\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, use_idf=True,\n",
       "                                 vocabulary=None)),\n",
       "                ('clf',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='auto', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#обучим наш пайплайн\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['INFP', 'INFP', 'INFP', 'INTP', 'INFP', 'INFP', 'INFP', 'INFJ',\n",
       "       'INTJ', 'INFJ'], dtype=object)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#наши прогнозы для тестовой выборки\n",
    "preds = pipeline.predict(X_test)\n",
    "preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_results = {\n",
    "    'approach': [],\n",
    "    'f1_score_micro': [],\n",
    "    'f1_score_macro': [],\n",
    "    'f1_score_weighted': [],\n",
    "    'precision_score_micro': [],\n",
    "    'precision_score_macro': [],\n",
    "    'precision_score_weighted': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'approach': ['Multi_LogReg'], 'f1_score_micro': [0.4734900875979714], 'f1_score_macro': [0.1888899973765767], 'f1_score_weighted': [0.4172112154271234], 'precision_score_micro': [0.4734900875979714], 'precision_score_macro': [0.3257913876389222], 'precision_score_weighted': [0.4715278973911036]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anakonda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "models_results['approach'].append('Multi_LogReg')\n",
    "models_results['f1_score_micro'].append(f1_score(y_test, preds, average='micro'))\n",
    "models_results['f1_score_macro'].append(f1_score(y_test, preds, average='macro'))\n",
    "models_results['f1_score_weighted'].append(f1_score(y_test, preds, average='weighted'))\n",
    "models_results['precision_score_micro'].append(precision_score(y_test, preds, average='micro'))\n",
    "models_results['precision_score_macro'].append(precision_score(y_test, preds, average='macro'))\n",
    "models_results['precision_score_weighted'].append(precision_score(y_test, preds, average='weighted'))\n",
    "\n",
    "print(models_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Многоклассовая классификация с с GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#соберем наш простой pipeline, но уже с другим классификатором\n",
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X[self.column]\n",
    "\n",
    "pipeline = Pipeline([('title_selector', FeatureSelector(column='posts')), \n",
    "                     ('title_tfidf', TfidfVectorizer()), \n",
    "                     ('clf', GradientBoostingClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('title_selector', FeatureSelector(column='posts')),\n",
       "                ('title_tfidf',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=N...\n",
       "                                            learning_rate=0.1, loss='deviance',\n",
       "                                            max_depth=3, max_features=None,\n",
       "                                            max_leaf_nodes=None,\n",
       "                                            min_impurity_decrease=0.0,\n",
       "                                            min_impurity_split=None,\n",
       "                                            min_samples_leaf=1,\n",
       "                                            min_samples_split=2,\n",
       "                                            min_weight_fraction_leaf=0.0,\n",
       "                                            n_estimators=100,\n",
       "                                            n_iter_no_change=None,\n",
       "                                            presort='deprecated',\n",
       "                                            random_state=None, subsample=1.0,\n",
       "                                            tol=0.0001, validation_fraction=0.1,\n",
       "                                            verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#обучим наш пайплайн\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['INTP', 'INFJ', 'ENFP', 'INTJ', 'ENFP', 'INFP', 'INFP', 'ISFJ',\n",
       "       'INTJ', 'INFJ'], dtype=object)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#наши прогнозы для тестовой выборки\n",
    "preds = pipeline.predict(X_test)\n",
    "preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'approach': ['Multi_LogReg', 'Multi_Grad_Boost'], 'f1_score_micro': [0.4734900875979714, 0.4698017519594283], 'f1_score_macro': [0.1888899973765767, 0.3095925559477476], 'f1_score_weighted': [0.4172112154271234, 0.46030986850417094], 'precision_score_micro': [0.4734900875979714, 0.4698017519594283], 'precision_score_macro': [0.3257913876389222, 0.34547026387509105], 'precision_score_weighted': [0.4715278973911036, 0.4699867494759581]}\n"
     ]
    }
   ],
   "source": [
    "models_results['approach'].append('Multi_Grad_Boost')\n",
    "models_results['f1_score_micro'].append(f1_score(y_test, preds, average='micro'))\n",
    "models_results['f1_score_macro'].append(f1_score(y_test, preds, average='macro'))\n",
    "models_results['f1_score_weighted'].append(f1_score(y_test, preds, average='weighted'))\n",
    "models_results['precision_score_micro'].append(precision_score(y_test, preds, average='micro'))\n",
    "models_results['precision_score_macro'].append(precision_score(y_test, preds, average='macro'))\n",
    "models_results['precision_score_weighted'].append(precision_score(y_test, preds, average='weighted'))\n",
    "\n",
    "print(models_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У GradientBoostingClassifier получились лучшие показатели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Многоклассовая классификация с RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#теперь соберем pipeline с RandomForestClassifier\n",
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X[self.column]\n",
    "\n",
    "pipeline = Pipeline([('title_selector', FeatureSelector(column='posts')), \n",
    "                     ('title_tfidf', TfidfVectorizer()), \n",
    "                     ('clf', RandomForestClassifier(n_estimators=1000))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#обучим наш пайплайн и запишем прогнозы\n",
    "pipeline.fit(X_train, y_train)\n",
    "preds = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'approach': ['Multi_LogReg', 'Multi_Grad_Boost', 'Multi_RandFor'], 'f1_score_micro': [0.4734900875979714, 0.4698017519594283, 0.33148916551406177], 'f1_score_macro': [0.1888899973765767, 0.3095925559477476, 0.09421681326234235], 'f1_score_weighted': [0.4172112154271234, 0.46030986850417094, 0.25297778825380945], 'precision_score_micro': [0.4734900875979714, 0.4698017519594283, 0.33148916551406177], 'precision_score_macro': [0.3257913876389222, 0.34547026387509105, 0.22372854536744724], 'precision_score_weighted': [0.4715278973911036, 0.4699867494759581, 0.43603957808592186]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anakonda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "models_results['approach'].append('Multi_RandFor')\n",
    "models_results['f1_score_micro'].append(f1_score(y_test, preds, average='micro'))\n",
    "models_results['f1_score_macro'].append(f1_score(y_test, preds, average='macro'))\n",
    "models_results['f1_score_weighted'].append(f1_score(y_test, preds, average='weighted'))\n",
    "models_results['precision_score_micro'].append(precision_score(y_test, preds, average='micro'))\n",
    "models_results['precision_score_macro'].append(precision_score(y_test, preds, average='macro'))\n",
    "models_results['precision_score_weighted'].append(precision_score(y_test, preds, average='weighted'))\n",
    "\n",
    "print(models_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Попробуем пойти другим путем. 16 бинарных подзадач с LogisticRegression\n",
    "\n",
    "Вместо решения задачи многоклассовой классификации попробуем разделить на 16 бинарных задач и выберем тот тип по итогу, вероятность которого больше всех."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENFJ</th>\n",
       "      <th>ENFP</th>\n",
       "      <th>ENTJ</th>\n",
       "      <th>ENTP</th>\n",
       "      <th>ESFJ</th>\n",
       "      <th>ESFP</th>\n",
       "      <th>ESTJ</th>\n",
       "      <th>ESTP</th>\n",
       "      <th>INFJ</th>\n",
       "      <th>INFP</th>\n",
       "      <th>INTJ</th>\n",
       "      <th>INTP</th>\n",
       "      <th>ISFJ</th>\n",
       "      <th>ISFP</th>\n",
       "      <th>ISTJ</th>\n",
       "      <th>ISTP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ENFJ  ENFP  ENTJ  ENTP  ESFJ  ESFP  ESTJ  ESTP  INFJ  INFP  INTJ  INTP  \\\n",
       "0     0     0     0     0     0     0     0     0     1     0     0     0   \n",
       "1     0     0     0     1     0     0     0     0     0     0     0     0   \n",
       "2     0     0     0     0     0     0     0     0     0     0     0     1   \n",
       "3     0     0     0     0     0     0     0     0     0     0     1     0   \n",
       "4     0     0     1     0     0     0     0     0     0     0     0     0   \n",
       "\n",
       "   ISFJ  ISFP  ISTJ  ISTP  \n",
       "0     0     0     0     0  \n",
       "1     0     0     0     0  \n",
       "2     0     0     0     0  \n",
       "3     0     0     0     0  \n",
       "4     0     0     0     0  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "types = pd.get_dummies(comments['type'])\n",
    "types.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ENFJ', 'ENFP', 'ENTJ', 'ENTP', 'ESFJ', 'ESFP', 'ESTJ', 'ESTP', 'INFJ', 'INFP', 'INTJ', 'INTP', 'ISFJ', 'ISFP', 'ISTJ', 'ISTP']\n"
     ]
    }
   ],
   "source": [
    "list_types = list(types.columns)\n",
    "print(list_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posts</th>\n",
       "      <th>ENFJ</th>\n",
       "      <th>ENFP</th>\n",
       "      <th>ENTJ</th>\n",
       "      <th>ENTP</th>\n",
       "      <th>ESFJ</th>\n",
       "      <th>ESFP</th>\n",
       "      <th>ESTJ</th>\n",
       "      <th>ESTP</th>\n",
       "      <th>INFJ</th>\n",
       "      <th>INFP</th>\n",
       "      <th>INTJ</th>\n",
       "      <th>INTP</th>\n",
       "      <th>ISFJ</th>\n",
       "      <th>ISFP</th>\n",
       "      <th>ISTJ</th>\n",
       "      <th>ISTP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>moments sportscenter plays pranks lifechanging...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>finding lack posts alarming sex boring positio...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>good blessing curse absolutely positive friend...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dear enjoyed conversation day esoteric gabbing...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fired silly misconception approaching logicall...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               posts  ENFJ  ENFP  ENTJ  ENTP  \\\n",
       "0  moments sportscenter plays pranks lifechanging...     0     0     0     0   \n",
       "1  finding lack posts alarming sex boring positio...     0     0     0     1   \n",
       "2  good blessing curse absolutely positive friend...     0     0     0     0   \n",
       "3  dear enjoyed conversation day esoteric gabbing...     0     0     0     0   \n",
       "4  fired silly misconception approaching logicall...     0     0     1     0   \n",
       "\n",
       "   ESFJ  ESFP  ESTJ  ESTP  INFJ  INFP  INTJ  INTP  ISFJ  ISFP  ISTJ  ISTP  \n",
       "0     0     0     0     0     1     0     0     0     0     0     0     0  \n",
       "1     0     0     0     0     0     0     0     0     0     0     0     0  \n",
       "2     0     0     0     0     0     0     0     1     0     0     0     0  \n",
       "3     0     0     0     0     0     0     1     0     0     0     0     0  \n",
       "4     0     0     0     0     0     0     0     0     0     0     0     0  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_binary = pd.concat([comments['posts'], types], axis=1)\n",
    "comments_binary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(comments_binary, comments_binary[list_types], random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# в цикле обучим pipeline с LogisticRegression, добавляя новую колонку в  df_prob с вероятностью отнесения данного пользователя к определенному типу\n",
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X[self.column]\n",
    "\n",
    "df_prob = pd.DataFrame()\n",
    "\n",
    "for i in list_types:\n",
    "    \n",
    "    pipeline = Pipeline([('title_selector', FeatureSelector(column='posts')), \n",
    "                         ('title_tfidf', TfidfVectorizer()), \n",
    "                         ('clf', LogisticRegression())])\n",
    "    pipeline.fit(X_train, y_train[i])\n",
    "    df_prob[i] = pipeline.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_test нам нужно вернуть в исходное состояние (до get_dummie) для сравнения с прогнозом. Для этого воспользуемся функцией decode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(row):\n",
    "    for c in y_test.columns:\n",
    "        if row[c] == 1:\n",
    "            return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4587    ISFP\n",
       "2786    INFJ\n",
       "2813    ENFP\n",
       "3705    INTP\n",
       "5957    ISFP\n",
       "        ... \n",
       "7256    ISFP\n",
       "2645    ENTP\n",
       "4773    ISTP\n",
       "7242    INFP\n",
       "6523    INFJ\n",
       "Length: 2169, dtype: object"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_multi = y_test.apply(decode, axis=1)\n",
    "y_test_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENFJ</th>\n",
       "      <th>ENFP</th>\n",
       "      <th>ENTJ</th>\n",
       "      <th>ENTP</th>\n",
       "      <th>ESFJ</th>\n",
       "      <th>ESFP</th>\n",
       "      <th>ESTJ</th>\n",
       "      <th>ESTP</th>\n",
       "      <th>INFJ</th>\n",
       "      <th>INFP</th>\n",
       "      <th>INTJ</th>\n",
       "      <th>INTP</th>\n",
       "      <th>ISFJ</th>\n",
       "      <th>ISFP</th>\n",
       "      <th>ISTJ</th>\n",
       "      <th>ISTP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.024636</td>\n",
       "      <td>0.087665</td>\n",
       "      <td>0.022872</td>\n",
       "      <td>0.047351</td>\n",
       "      <td>0.003814</td>\n",
       "      <td>0.006513</td>\n",
       "      <td>0.004064</td>\n",
       "      <td>0.008422</td>\n",
       "      <td>0.160376</td>\n",
       "      <td>0.269963</td>\n",
       "      <td>0.061839</td>\n",
       "      <td>0.097951</td>\n",
       "      <td>0.022631</td>\n",
       "      <td>0.031750</td>\n",
       "      <td>0.031102</td>\n",
       "      <td>0.032893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.033260</td>\n",
       "      <td>0.114280</td>\n",
       "      <td>0.015315</td>\n",
       "      <td>0.042538</td>\n",
       "      <td>0.004040</td>\n",
       "      <td>0.004870</td>\n",
       "      <td>0.004281</td>\n",
       "      <td>0.007677</td>\n",
       "      <td>0.199900</td>\n",
       "      <td>0.328585</td>\n",
       "      <td>0.075350</td>\n",
       "      <td>0.048405</td>\n",
       "      <td>0.021498</td>\n",
       "      <td>0.027037</td>\n",
       "      <td>0.019714</td>\n",
       "      <td>0.023968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.025590</td>\n",
       "      <td>0.251114</td>\n",
       "      <td>0.022937</td>\n",
       "      <td>0.026617</td>\n",
       "      <td>0.003782</td>\n",
       "      <td>0.005482</td>\n",
       "      <td>0.004016</td>\n",
       "      <td>0.009444</td>\n",
       "      <td>0.219686</td>\n",
       "      <td>0.282922</td>\n",
       "      <td>0.106672</td>\n",
       "      <td>0.021301</td>\n",
       "      <td>0.018505</td>\n",
       "      <td>0.028482</td>\n",
       "      <td>0.023056</td>\n",
       "      <td>0.027855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.017657</td>\n",
       "      <td>0.029069</td>\n",
       "      <td>0.023216</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.004120</td>\n",
       "      <td>0.005030</td>\n",
       "      <td>0.004365</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.079070</td>\n",
       "      <td>0.100114</td>\n",
       "      <td>0.190354</td>\n",
       "      <td>0.344625</td>\n",
       "      <td>0.016501</td>\n",
       "      <td>0.019668</td>\n",
       "      <td>0.027364</td>\n",
       "      <td>0.052098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.026842</td>\n",
       "      <td>0.176722</td>\n",
       "      <td>0.022939</td>\n",
       "      <td>0.080773</td>\n",
       "      <td>0.004361</td>\n",
       "      <td>0.006038</td>\n",
       "      <td>0.004663</td>\n",
       "      <td>0.010947</td>\n",
       "      <td>0.165552</td>\n",
       "      <td>0.194345</td>\n",
       "      <td>0.050182</td>\n",
       "      <td>0.073032</td>\n",
       "      <td>0.023291</td>\n",
       "      <td>0.042548</td>\n",
       "      <td>0.016626</td>\n",
       "      <td>0.035307</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ENFJ      ENFP      ENTJ      ENTP      ESFJ      ESFP      ESTJ  \\\n",
       "0  0.024636  0.087665  0.022872  0.047351  0.003814  0.006513  0.004064   \n",
       "1  0.033260  0.114280  0.015315  0.042538  0.004040  0.004870  0.004281   \n",
       "2  0.025590  0.251114  0.022937  0.026617  0.003782  0.005482  0.004016   \n",
       "3  0.017657  0.029069  0.023216  0.065000  0.004120  0.005030  0.004365   \n",
       "4  0.026842  0.176722  0.022939  0.080773  0.004361  0.006038  0.004663   \n",
       "\n",
       "       ESTP      INFJ      INFP      INTJ      INTP      ISFJ      ISFP  \\\n",
       "0  0.008422  0.160376  0.269963  0.061839  0.097951  0.022631  0.031750   \n",
       "1  0.007677  0.199900  0.328585  0.075350  0.048405  0.021498  0.027037   \n",
       "2  0.009444  0.219686  0.282922  0.106672  0.021301  0.018505  0.028482   \n",
       "3  0.009766  0.079070  0.100114  0.190354  0.344625  0.016501  0.019668   \n",
       "4  0.010947  0.165552  0.194345  0.050182  0.073032  0.023291  0.042548   \n",
       "\n",
       "       ISTJ      ISTP  \n",
       "0  0.031102  0.032893  \n",
       "1  0.019714  0.023968  \n",
       "2  0.023056  0.027855  \n",
       "3  0.027364  0.052098  \n",
       "4  0.016626  0.035307  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prob.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нам нужно выделить название колонки с максимальной вероятностью"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       INFP\n",
       "1       INFP\n",
       "2       INFP\n",
       "3       INTP\n",
       "4       INFP\n",
       "        ... \n",
       "2164    INFP\n",
       "2165    ENTP\n",
       "2166    INFP\n",
       "2167    INFP\n",
       "2168    INFJ\n",
       "Name: predict, Length: 2169, dtype: object"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prob['predict'] = df_prob.columns[df_prob.values.argsort(1)[:, -1]]\n",
    "df_prob['predict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'approach': ['Multi_LogReg', 'Multi_Grad_Boost', 'Multi_RandFor', 'Binary_LogReg', 'Binary_LogReg'], 'f1_score_micro': [0.4734900875979714, 0.4698017519594283, 0.33148916551406177, 0.4633471645919779], 'f1_score_macro': [0.1888899973765767, 0.3095925559477476, 0.09421681326234235, 0.1814749363220307], 'f1_score_weighted': [0.4172112154271234, 0.46030986850417094, 0.25297778825380945, 0.4057896608579111], 'precision_score_micro': [0.4734900875979714, 0.4698017519594283, 0.33148916551406177, 0.4633471645919779], 'precision_score_macro': [0.3257913876389222, 0.34547026387509105, 0.22372854536744724, 0.3289755182357287], 'precision_score_weighted': [0.4715278973911036, 0.4699867494759581, 0.43603957808592186, 0.4731197404661596]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anakonda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "models_results['approach'].append('Binary_LogReg')\n",
    "models_results['f1_score_micro'].append(f1_score(y_test_multi, df_prob['predict'], average='micro'))\n",
    "models_results['f1_score_macro'].append(f1_score(y_test_multi, df_prob['predict'], average='macro'))\n",
    "models_results['f1_score_weighted'].append(f1_score(y_test_multi, df_prob['predict'], average='weighted'))\n",
    "models_results['precision_score_micro'].append(precision_score(y_test_multi, df_prob['predict'], average='micro'))\n",
    "models_results['precision_score_macro'].append(precision_score(y_test_multi, df_prob['predict'], average='macro'))\n",
    "models_results['precision_score_weighted'].append(precision_score(y_test_multi, df_prob['predict'], average='weighted'))\n",
    "\n",
    "print(models_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Теперь попробуем с GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(comments_binary, comments_binary[list_types], random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Теперь также в цикле обучаем GradientBoostingClassifier\n",
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X[self.column]\n",
    "\n",
    "df_prob = pd.DataFrame()\n",
    "\n",
    "for i in list_types:\n",
    "    \n",
    "    pipeline = Pipeline([('title_selector', FeatureSelector(column='posts')), \n",
    "                         ('title_tfidf', TfidfVectorizer()), \n",
    "                         ('clf', GradientBoostingClassifier())])\n",
    "    pipeline.fit(X_train, y_train[i])\n",
    "    df_prob[i] = pipeline.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENFJ</th>\n",
       "      <th>ENFP</th>\n",
       "      <th>ENTJ</th>\n",
       "      <th>ENTP</th>\n",
       "      <th>ESFJ</th>\n",
       "      <th>ESFP</th>\n",
       "      <th>ESTJ</th>\n",
       "      <th>ESTP</th>\n",
       "      <th>INFJ</th>\n",
       "      <th>INFP</th>\n",
       "      <th>INTJ</th>\n",
       "      <th>INTP</th>\n",
       "      <th>ISFJ</th>\n",
       "      <th>ISFP</th>\n",
       "      <th>ISTJ</th>\n",
       "      <th>ISTP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00372</td>\n",
       "      <td>0.048378</td>\n",
       "      <td>0.007618</td>\n",
       "      <td>0.028297</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>7.294238e-07</td>\n",
       "      <td>0.00026</td>\n",
       "      <td>0.097797</td>\n",
       "      <td>0.218575</td>\n",
       "      <td>0.055623</td>\n",
       "      <td>0.273383</td>\n",
       "      <td>0.002577</td>\n",
       "      <td>0.009175</td>\n",
       "      <td>0.005609</td>\n",
       "      <td>0.015284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00372</td>\n",
       "      <td>0.033593</td>\n",
       "      <td>0.007618</td>\n",
       "      <td>0.028297</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>7.294238e-07</td>\n",
       "      <td>0.00026</td>\n",
       "      <td>0.425311</td>\n",
       "      <td>0.103165</td>\n",
       "      <td>0.044921</td>\n",
       "      <td>0.038749</td>\n",
       "      <td>0.002577</td>\n",
       "      <td>0.009175</td>\n",
       "      <td>0.005609</td>\n",
       "      <td>0.015284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00372</td>\n",
       "      <td>0.676895</td>\n",
       "      <td>0.007618</td>\n",
       "      <td>0.028297</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>7.294238e-07</td>\n",
       "      <td>0.00026</td>\n",
       "      <td>0.121794</td>\n",
       "      <td>0.149638</td>\n",
       "      <td>0.332135</td>\n",
       "      <td>0.050254</td>\n",
       "      <td>0.002577</td>\n",
       "      <td>0.009175</td>\n",
       "      <td>0.005609</td>\n",
       "      <td>0.015284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00372</td>\n",
       "      <td>0.032095</td>\n",
       "      <td>0.007618</td>\n",
       "      <td>0.049814</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>7.294238e-07</td>\n",
       "      <td>0.00026</td>\n",
       "      <td>0.066874</td>\n",
       "      <td>0.086341</td>\n",
       "      <td>0.265972</td>\n",
       "      <td>0.556502</td>\n",
       "      <td>0.002577</td>\n",
       "      <td>0.009175</td>\n",
       "      <td>0.005609</td>\n",
       "      <td>0.071480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00372</td>\n",
       "      <td>0.301662</td>\n",
       "      <td>0.007618</td>\n",
       "      <td>0.059911</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>7.294238e-07</td>\n",
       "      <td>0.00026</td>\n",
       "      <td>0.097322</td>\n",
       "      <td>0.071950</td>\n",
       "      <td>0.043868</td>\n",
       "      <td>0.061104</td>\n",
       "      <td>0.002577</td>\n",
       "      <td>0.009175</td>\n",
       "      <td>0.005609</td>\n",
       "      <td>0.015284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ENFJ      ENFP      ENTJ      ENTP      ESFJ      ESFP          ESTJ  \\\n",
       "0  0.00372  0.048378  0.007618  0.028297  0.000004  0.000011  7.294238e-07   \n",
       "1  0.00372  0.033593  0.007618  0.028297  0.000004  0.000011  7.294238e-07   \n",
       "2  0.00372  0.676895  0.007618  0.028297  0.000004  0.000011  7.294238e-07   \n",
       "3  0.00372  0.032095  0.007618  0.049814  0.000004  0.000011  7.294238e-07   \n",
       "4  0.00372  0.301662  0.007618  0.059911  0.000004  0.000011  7.294238e-07   \n",
       "\n",
       "      ESTP      INFJ      INFP      INTJ      INTP      ISFJ      ISFP  \\\n",
       "0  0.00026  0.097797  0.218575  0.055623  0.273383  0.002577  0.009175   \n",
       "1  0.00026  0.425311  0.103165  0.044921  0.038749  0.002577  0.009175   \n",
       "2  0.00026  0.121794  0.149638  0.332135  0.050254  0.002577  0.009175   \n",
       "3  0.00026  0.066874  0.086341  0.265972  0.556502  0.002577  0.009175   \n",
       "4  0.00026  0.097322  0.071950  0.043868  0.061104  0.002577  0.009175   \n",
       "\n",
       "       ISTJ      ISTP  \n",
       "0  0.005609  0.015284  \n",
       "1  0.005609  0.015284  \n",
       "2  0.005609  0.015284  \n",
       "3  0.005609  0.071480  \n",
       "4  0.005609  0.015284  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_multi = y_test.apply(decode, axis=1)\n",
    "df_prob.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       INTP\n",
       "1       INFJ\n",
       "2       ENFP\n",
       "3       INTP\n",
       "4       ENFP\n",
       "        ... \n",
       "2164    ESTJ\n",
       "2165    ENTP\n",
       "2166    INFP\n",
       "2167    INFP\n",
       "2168    INFJ\n",
       "Name: predict, Length: 2169, dtype: object"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prob['predict'] = df_prob.columns[df_prob.values.argsort(1)[:, -1]]\n",
    "df_prob['predict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models_results['approach'].append('Binary_GradBoost')\n",
    "models_results['f1_score_micro'].append(f1_score(y_test_multi, df_prob['predict'], average='micro'))\n",
    "models_results['f1_score_macro'].append(f1_score(y_test_multi, df_prob['predict'], average='macro'))\n",
    "models_results['f1_score_weighted'].append(f1_score(y_test_multi, df_prob['predict'], average='weighted'))\n",
    "models_results['precision_score_micro'].append(precision_score(y_test_multi, df_prob['predict'], average='micro'))\n",
    "models_results['precision_score_macro'].append(precision_score(y_test_multi, df_prob['predict'], average='macro'))\n",
    "models_results['precision_score_weighted'].append(precision_score(y_test_multi, df_prob['predict'], average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'approach': ['Multi_LogReg',\n",
       "  'Multi_Grad_Boost',\n",
       "  'Multi_RandFor',\n",
       "  'Binary_LogReg',\n",
       "  'Binary_LogReg',\n",
       "  'Binary_GradBoost'],\n",
       " 'f1_score_micro': [0.4734900875979714,\n",
       "  0.4698017519594283,\n",
       "  0.33148916551406177,\n",
       "  0.4633471645919779,\n",
       "  0.45550945136007376],\n",
       " 'f1_score_macro': [0.1888899973765767,\n",
       "  0.3095925559477476,\n",
       "  0.09421681326234235,\n",
       "  0.1814749363220307,\n",
       "  0.2875672552887848],\n",
       " 'f1_score_weighted': [0.4172112154271234,\n",
       "  0.46030986850417094,\n",
       "  0.25297778825380945,\n",
       "  0.4057896608579111,\n",
       "  0.4448779214022036],\n",
       " 'precision_score_micro': [0.4734900875979714,\n",
       "  0.4698017519594283,\n",
       "  0.33148916551406177,\n",
       "  0.4633471645919779,\n",
       "  0.45550945136007376],\n",
       " 'precision_score_macro': [0.3257913876389222,\n",
       "  0.34547026387509105,\n",
       "  0.22372854536744724,\n",
       "  0.3289755182357287,\n",
       "  0.3158317851912673],\n",
       " 'precision_score_weighted': [0.4715278973911036,\n",
       "  0.4699867494759581,\n",
       "  0.43603957808592186,\n",
       "  0.4731197404661596,\n",
       "  0.4561331838159128]}"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>approach</th>\n",
       "      <th>f1_score_micro</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>f1_score_weighted</th>\n",
       "      <th>precision_score_micro</th>\n",
       "      <th>precision_score_macro</th>\n",
       "      <th>precision_score_weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Multi_LogReg</td>\n",
       "      <td>0.473490</td>\n",
       "      <td>0.188890</td>\n",
       "      <td>0.417211</td>\n",
       "      <td>0.473490</td>\n",
       "      <td>0.325791</td>\n",
       "      <td>0.471528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Multi_Grad_Boost</td>\n",
       "      <td>0.469802</td>\n",
       "      <td>0.309593</td>\n",
       "      <td>0.460310</td>\n",
       "      <td>0.469802</td>\n",
       "      <td>0.345470</td>\n",
       "      <td>0.469987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Multi_RandFor</td>\n",
       "      <td>0.331489</td>\n",
       "      <td>0.094217</td>\n",
       "      <td>0.252978</td>\n",
       "      <td>0.331489</td>\n",
       "      <td>0.223729</td>\n",
       "      <td>0.436040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Binary_LogReg</td>\n",
       "      <td>0.463347</td>\n",
       "      <td>0.181475</td>\n",
       "      <td>0.405790</td>\n",
       "      <td>0.463347</td>\n",
       "      <td>0.328976</td>\n",
       "      <td>0.473120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Binary_GradBoost</td>\n",
       "      <td>0.455509</td>\n",
       "      <td>0.287567</td>\n",
       "      <td>0.444878</td>\n",
       "      <td>0.455509</td>\n",
       "      <td>0.315832</td>\n",
       "      <td>0.456133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           approach  f1_score_micro  f1_score_macro  f1_score_weighted  \\\n",
       "0      Multi_LogReg        0.473490        0.188890           0.417211   \n",
       "1  Multi_Grad_Boost        0.469802        0.309593           0.460310   \n",
       "2     Multi_RandFor        0.331489        0.094217           0.252978   \n",
       "3     Binary_LogReg        0.463347        0.181475           0.405790   \n",
       "4  Binary_GradBoost        0.455509        0.287567           0.444878   \n",
       "\n",
       "   precision_score_micro  precision_score_macro  precision_score_weighted  \n",
       "0               0.473490               0.325791                  0.471528  \n",
       "1               0.469802               0.345470                  0.469987  \n",
       "2               0.331489               0.223729                  0.436040  \n",
       "3               0.463347               0.328976                  0.473120  \n",
       "4               0.455509               0.315832                  0.456133  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data=models_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
